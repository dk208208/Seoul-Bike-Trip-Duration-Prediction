# -*- coding: utf-8 -*-
"""ML_GUIDED_PROJECT_01.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/126S_lRn8HynKM6DTP16asOhBImufk25m
"""

# Commented out IPython magic to ensure Python compatibility.
# Importing Important Libraries
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

from sklearn.metrics.pairwise import haversine_distances
from math import radians, cos, sin, asin, sqrt
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestRegressor

from sklearn.metrics import classification_report, mean_absolute_error, mean_squared_error, r2_score

from google.colab import drive
drive.mount('/content/drive')

# Importing Dataset
path = "/content/drive/My Drive/Colab Notebooks/For_modeling.csv"
seoul = pd.read_csv(path, index_col=[0])
seoul.head()

# Checking Data Types
seoul.dtypes

"""**Exploratory Data Analysis**"""

# Checking Null Values
seoul.isna().sum()

"""There is no null value"""

seoul.shape

# Data Information
seoul.info()

# Data Describe
seoul.describe()

# No. of unique values in dataset
seoul.nunique().sort_values(ascending=False)

"""## Data Visualization"""

sns.set(rc={"figure.figsize": (9,6.5)})
sns.distplot(seoul['Duration'],bins=15)
sns.histplot()

figs = plt.figure(figsize=(15,7))
ax1 = figs.add_subplot(121)
ax2 = figs.add_subplot(122)
x = seoul["Duration"]
ax1.hist(x)
ax2.boxplot(x);

sns.set(rc={"figure.figsize": (9,6.5)})
sns.distplot(seoul['Distance'],bins=15)

figs = plt.figure(figsize=(15,7))
ax1 = figs.add_subplot(121)
ax2 = figs.add_subplot(122)
x = seoul["Distance"]
ax1.hist(x)
ax2.boxplot(x);

seoul['Distance'].value_counts()

figs = plt.figure(figsize=(15,7))
ax1 = figs.add_subplot(121)
ax2 = figs.add_subplot(122)
x = seoul["Haversine"]
ax1.hist(x)
ax2.boxplot(x);

seoul['Distance'].value_counts()

#  Checking 0.00 values
(seoul == 0.00).sum()

"""A lot of columns have zero(0.00) value

Also, the **Latitude and Longitude columns** are titles **incorrectly**, as Latitude cannot be greater than 90Â°
"""

print(seoul["PLatd"][1])
print(seoul["PLong"][1])
print(seoul["DLatd"][1])
print(seoul["DLong"][1])

seoul.columns

seoul[['PLatd','PLong']] = seoul[['PLong','PLatd']]
seoul[['DLatd','DLong']] = seoul[['DLong','DLatd']]
seoul.head()

"""Trying to fill Haversine distance using function"""

def calc_haversine(lon1, lat1, lon2, lat2, is_deg=True):
    """
    Calculate the great circle distance between two points 
    on the earth (specified in decimal degrees)
    """
    # convert decimal degrees to radians
    if is_deg:
        lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])
    # haversine formula 
    dlon = lon2 - lon1 
    dlat = lat2 - lat1 
    a = (sin(dlat/2)**2) + ((cos(lat1) * cos(lat2)) * sin(dlon/2)**2)
    c = 2 * asin(sqrt(a)) 
    # Radius of earth in kilometers is 6371
    km = 6371* c
    return km

d = seoul["Haversine"][1]
plat = seoul["PLatd"][1]
plon = seoul["PLong"][1]
dlat = seoul["DLatd"][1]
dlon = seoul["DLong"][1]

print(d)
calc_haversine(plon, plat, dlon, dlat)

"""Finding Outliers"""

from scipy import stats

z = np.abs(stats.zscore(seoul))
print(z)

threshold = 3
print(np.where(z>3))

seoul_data = seoul[(z < 3).all(axis=1)]
seoul_data.shape

seoul.shape

"""**Preprocessing**"""

# correlation

sns.set()
plt.figure(figsize=(15,10))
sns.heatmap(seoul.corr(), annot=False, cmap=plt.cm.CMRmap_r)
plt.show()

seoul_data.reset_index(drop=True, inplace=True)
seoul_data.tail()

ids = list(np.where(seoul_data["Haversine"] == 0.00)[0])
len(ids)

for s in ids[:10]:
    print(calc_haversine(seoul_data["PLong"][s], seoul_data["PLatd"][s], seoul_data["DLong"][s], seoul_data["DLatd"][s]))

"""Dropping the rows where Haversine is 0.00 along with other columns selected analytically"""

seoul_data.drop(labels=["Snow", "Precip", "PLatd", "PLong", "DLatd", "DLong"], axis=1, inplace=True)
seoul_data = seoul_data.loc[seoul_data["Dust"] * seoul_data["Wind"] * seoul_data["Haversine"] * seoul_data["Solar"]!= 0.0]
seoul_data.reset_index(drop=True, inplace=True)

# seoul_data

"""Dropping Day, Month and Week variables of drop context, as the
entire circulation is within the same city, and these columns become repetitive
"""

seoul_data.drop(labels=["Dday", "Dmonth", "DDweek"], axis=1, inplace=True)
seoul_data.tail()

skew = seoul_data.skew()
print(skew)

"""Multivariate Analysis"""

plt.figure(figsize=(15,10))
sns.set()
sns.heatmap(seoul_data.corr(), annot=True, cmap=plt.cm.CMRmap_r)
plt.show()

sampling = seoul_data.sample(n=10000)
sns.set(font_scale=3)
sns.pairplot(sampling, diag_kind='kde');

"""**Feature Engineering**"""

# 4925522
seoul_data_sample = seoul_data.sample(n=49255, replace=True,random_state=101)
seoul_data_sample.reset_index(drop=True,inplace=True)
seoul_data_sample

"""**splitting data for training and testing**"""

X = seoul_data_sample.drop(labels=["Duration"], axis = 1)
y = seoul_data_sample["Duration"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=101)

# Feature selection
# use RandomForest Regression

rfe = RFE(estimator=RandomForestRegressor(), n_features_to_select=10)
X_train_rfe = rfe.fit_transform(X_train,y_train)
X_test_rfe = rfe.transform(X_test)

sns.set()
plt.figure(figsize=(7,5))
sns.barplot(y = X_train.columns, x = max(rfe.ranking_) - rfe.ranking_);

feature_list_rfe = [col for i,col in enumerate(X_train.columns) if rfe.support_[i]]
feature_list_rfe

"""Modelling"""

X_train_rfe.shape

X_test_rfe.shape

y_train.shape

y_test.shape

rf = RandomForestRegressor(n_estimators=50, n_jobs=-1)

rf.fit(X_train_rfe, y_train)
y_hat_test = rf.predict(X_test_rfe)
y_hat_train = rf.predict(X_train_rfe)

y_hat_train

print(f'Training score : {rf.score(X_train_rfe, y_train)}')

print()
print('r2 score:', r2_score(y_test, y_hat_test))
print('MAE:', mean_absolute_error(y_test, y_hat_test))
print('MSE:', mean_squared_error(y_test, y_hat_test))
print('RMSE:', np.sqrt(mean_squared_error(y_test, y_hat_test)))

plt.figure(figsize = (7,5))
sns.distplot(y_test - y_hat_test)
plt.title("Error Rate Distribution");

"""**Feature Importance**"""

plt.figure(figsize=(7,5))
sns.barplot(x = rf.feature_importances_, y = feature_list_rfe)
plt.xlabel('Feature Importance Score')
plt.ylabel('Features')
plt.title("Visualizing Important Features", size=14);

"""**Hyperparametere Tuning**"""

from sklearn.model_selection import RandomizedSearchCV
from pprint import pprint 

# Number of trees in random forest
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]
# Number of features to consider at every split
max_features = ['auto', 'sqrt']
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [2, 5, 10]
# Minimum number of samples required at each leaf node
min_samples_leaf = [1, 2, 4]
# Method of selecting samples for training each tree
bootstrap = [True, False]
# Create the random grid
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}
pprint(random_grid)

params = {'bootstrap': [True, False],
 'max_depth': [10, 50, 100,None],
 'max_features': ['auto', 'sqrt'],
 'min_samples_leaf': [1, 2, 4],
 'min_samples_split': [2, 5, 10],
 'n_estimators': [200, 1000, 2000]}

# Use the random grid to search for best hyperparameters
# Random search of parameters, using 3 fold cross validation, 
# search across 100 different combinations, and use all available cores

rf_random = RandomizedSearchCV(estimator = rf, param_distributions = params, n_iter = 1, cv = 3, verbose=2, random_state=42, n_jobs = -1)

# Fit the random search model
rf_random.fit(X_train_rfe, y_train)

rf_random.best_estimator_

# from joblib import dump, load
# dump(rf, 'For_Modelling.joblib')

# from sklearn.externals import joblib
import joblib  
joblib.dump(rf,'For_Modelling.joblib',compress = 9)

from google.colab import files
files.download('For_Modelling.joblib')